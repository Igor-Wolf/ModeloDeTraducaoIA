{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c140e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelos Ollama são otimizados então é possivel rodar modleos maiores utilizei o 12b\n",
    "\n",
    "SOURCE_LANG= \"English\"\n",
    "SOURCE_CODE= \"en\"\n",
    "TARGET_LANG= \"Portuguese\"\n",
    "TARGET_CODE= \"pt-BR\"\n",
    "TEXT= \"He wasn't always this bad, was he? Bayrd thought. He wanted the throne for his wife, but what lord wouldn’t? It was hard to look past the name. Bayrd’s family had followed the Sarand family with reverence for generations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ace41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tradução Local (Ollama) ---\n",
      "Ele não sempre foi assim, certo? Bayrd pensou. Ele queria o trono para sua esposa, mas qual nobre não desejaria? Era difícil ignorar o sobrenome. A família de Bayrd havia seguido a família Sarand com reverência por gerações."
     ]
    }
   ],
   "source": [
    "#Modo chat\n",
    "\n",
    "import ollama\n",
    "\n",
    "# Montando o prompt exatamente como a documentação pede (usando f-string)\n",
    "prompt = f\"\"\"You are a professional {SOURCE_LANG} ({SOURCE_CODE}) to {TARGET_LANG} ({TARGET_CODE}) translator. Your goal is to accurately convey the meaning and nuances of the original {source_lang} text while adhering to {target_lang} grammar, vocabulary, and cultural sensitivities.\n",
    "Produce only the {target_lang} translation, without any additional explanations or commentary. Please translate the following {SOURCE_LANG} text into {TARGET_LANG}:\n",
    "\n",
    "\n",
    "{TEXT}\"\"\"\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='translategemma:12b',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(f\"--- Tradução Local (Ollama) ---\")\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9649922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ele não sempre foi assim, não é mesmo? pensou Bayrd. Ele queria o trono para sua esposa, mas qual nobre não o desejaria? Era difícil ignorar o sobrenome. A família de Bayrd havia seguido a família Sarand com reverência por gerações."
     ]
    }
   ],
   "source": [
    "# Modo Geração\n",
    "\n",
    "import ollama\n",
    "\n",
    "# Aqui você manda o prompt direto como uma string\n",
    "stream = ollama.generate(\n",
    "    model='translategemma:12b',\n",
    "    prompt=prompt,  \n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['response'], end='', flush=True) # No generate, a chave é 'response' em vez de 'message'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b788b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tradução via API Request ---\n",
      "Ele não sempre foi assim, certo? Pensou Bayrd. Ele queria o trono para sua esposa, mas qual nobre não desejaria? Era difícil ignorar o sobrenome. A família de Bayrd havia seguido a família Sarand com reverência por gerações."
     ]
    }
   ],
   "source": [
    "#Modo API com request\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# O endereço padrão do Ollama no Windows\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# O corpo da requisição (JSON)\n",
    "payload = {\n",
    "    \"model\": \"translategemma:12b\",\n",
    "    \"prompt\": prompt, # Aquele prompt oficial que montamos\n",
    "    \"stream\": True    # Se False, ele espera a tradução toda antes de responder\n",
    "}\n",
    "\n",
    "# Fazendo a requisição com stream\n",
    "response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "print(f\"--- Tradução via API Request ---\")\n",
    "\n",
    "# Processando a resposta linha por linha (já que é um stream)\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        # O Ollama envia cada pedaço como um objeto JSON separado\n",
    "        chunk = json.loads(line)\n",
    "        print(chunk.get(\"response\", \"\"), end=\"\", flush=True)\n",
    "        \n",
    "        # O Ollama avisa quando terminou\n",
    "        if chunk.get(\"done\"):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
