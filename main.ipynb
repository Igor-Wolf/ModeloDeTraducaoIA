{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51824ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\aaa\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Utilizando o modelo diretamente fica pesado então foi necesário utilizar o 4b\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c172804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "CUDA disponível: True\n",
      "GPU detectada: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(f\"CUDA disponível: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU detectada: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30dce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
      "Loading weights: 100%|██████████| 883/883 [00:42<00:00, 21.00it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]                      \n"
     ]
    }
   ],
   "source": [
    "model_id = \"../Huggingface/hub/models--google--translategemma-4b-it/snapshots/10042cb0e6e7fdce748996a71dc3dc432a4e0c89\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "407f04f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ele não sempre foi assim, não é? Bayrd pensou. Ele queria o trono para sua esposa, mas qual nobre não gostaria? Era difícil ignorar o nome. A família de Bayrd sempre acompanhou a família Sarand com respeito, por gerações.\n"
     ]
    }
   ],
   "source": [
    "# ---- Text Translation ----\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"source_lang_code\": \"en\",\n",
    "                \"target_lang_code\": \"pt-BR\",\n",
    "                \"text\": \"He wasn't always this bad, was he? Bayrd thought. He wanted the throne for his wife, but what lord wouldn’t? It was hard to look past the name. Bayrd’s family had followed the Sarand family with reverence for generations\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(\n",
    "        **inputs, \n",
    "        do_sample=False,\n",
    "        max_new_tokens=512, \n",
    "        pad_token_id=processor.tokenizer.pad_token_id  \n",
    "    )\n",
    "\n",
    "generation = generation[0][input_len:]\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efdfa37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "model\n",
      "- Oi, Nancy. O que você está fazendo?\n",
      "Estou enviando um e-mail para um amigo.\n",
      "- Você usa e-mail com frequência?\n",
      "- Todos os dias. E você?\n",
      "- Eu também. Também navego na internet para notícias e esportes.\n",
      "- A internet é ótima, não é?\n",
      "- Com certeza. Nancy, você poderia me fazer um favor?\n",
      "Eu também gostaria de verificar meu e-mail.\n",
      "- Sem problemas. Darei um jeito em cerca de cinco minutos.\n",
      "- Obrigado.\n"
     ]
    }
   ],
   "source": [
    "# ---- Text Extraction and Translation ----\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"source_lang_code\": \"en\",\n",
    "                \"target_lang_code\": \"pt-BR\",\n",
    "                \"url\": \"https://uploads.tudosaladeaula.com/2024/09/HxOkgb9x-Sem20tC3ADtulo-3179-jpg.webp\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, \n",
    "                                do_sample=False,\n",
    "                                  max_new_tokens=512, \n",
    "                                pad_token_id=processor.tokenizer.pad_token_id  )\n",
    "\n",
    "generation = generation[0][input_len:]\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
